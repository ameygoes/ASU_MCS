======================================================================
============================ INTRODUCTION ============================
======================================================================
Methods from computer vision and natural language processing are used in both VQA and image captioning. Captioning also comprises providing a brief summary of the image by highlighting the essential characteristics, features, and connections between them in an image. To achieve this goal, the system must generate grammatically correct text that faithfully captures the syntax and semantics of the summary. Deep learning is an excellent technique for overcoming these challenges because of its layered design and range of architectures. Picture captioning, in essence, combines computer vision and natural language processing to recognize image components and generate human-readable words. Despite being an end-to-end process that uses an encoder to convert images into pixel sequences and a decoder to translate those sequences into descriptive sentences, image captioning still has a lot of challenges because it needs to mimic the human brain's ability to understand and describe images. In the part that follows, we will go deeper into these challenges.

Answering a question based on the image input is known as "visual question answering" (VQA). For the VQA problem to be fully solved, both modalities are required [6]. Existing VQA models only offer single-word responses, thus a model that can offer a series of word responses is necessary, particularly for medical images that need for in-depth explanations [6]. In this study, we investigate the performance of two well-known convolutional neural network (CNN) models for extracting visual features: densenet121 and Xception. Then, we'll employ straightforward encoder-decoder models and VisualBERT, a multimodal SOTA model. 	

The structure of this report is as follows. The section on related work offers an analysis of the research on medical picture captioning, VQA, and multitasking techniques that we discovered. The chest X-ray dataset and data pre-processing procedures are described in the dataset description part of our study. The methodology section gives a thorough explanation of the strategy utilized for the VQA and captioning of medical images. The findings of our research and tests are reported in the results section along with metrics for model evaluation. The summary of our study's major conclusions, its limitations, and future research directions are provided in the conclusion section.

======================================================================
============================ METHODOLOGY ============================
======================================================================
Convolutional neural networks are used to encode image features and LSTM to encode our prompt/question, and decoder models are used to produce the output in order to tackle the challenge of creating image captions and VQA tasks. The encoder and the decoder, which are neural networks trained in a supervised manner, are the two fundamental parts of the system. On the basis of the measures we employed, we trained various encoder architectures on the data set and chose the top performer. The characteristics produced by the encoder and the image captions are used to train a decoder, which creates image captions using the LSTM architecture. The parts we used in our system are described in full below.

======================================================================
============================ visual bert ============================
======================================================================
For problems involving both language and images, Visual BERT (VisBERT) is a multi-modal transformer architecture that combines the power of BERT with visual information. With the inclusion of a visual encoder that handles images, Visual BERT has a similar architecture to BERT. Three primary parts make up the Visual BERT model:

1. A text encoder: The text encoder processes textual input to create contextualized word embeddings and is based on the BERT architecture. These embeddings are created by BERT using a bidirectional transformer encoder, which are then utilized to create representations for the full sequence.
2. A visual encoder, which processes image input to create visual embeddings using a ResNet architecture. The ResNet is customized for the intended purpose after being pre-trained on the ImageNet dataset. In this instance, the CNN models' image features will be used as the section's input.
3. A multi-modal fusion layer, which blends text and visual embeddings to produce joint representations that integrate data from both modalities. The fusion layer comprises two attention mechanisms: self-attention, which enables the model to pay attention to various input sequence components, and cross-modal attention, which enables the model to attend both text and visual information when processing each modality.